{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install 'gymnasium[atari]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install atari-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install 'gymnasium[accept-rom-license]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc93057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03472700",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/SpaceInvaders-v5\",render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "height,width,channel = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0.7 ##epsilon\n",
    "n= random.random()\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(M):\n",
    "    \n",
    "    crop_img=[]\n",
    "    x=M\n",
    "    try:\n",
    "        \n",
    "        gray = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.pyrDown(gray)\n",
    "        return crop_img.append(gray[4:98,0:80])\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        gray = cv2.cvtColor(x[0], cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.pyrDown(gray)\n",
    "        return crop_img.append(gray[4:98,0:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ab79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(srb[1][3], cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing of image function called inside replay buffer\n",
    "#and to be used everytime before passing into neural net model\n",
    "\n",
    "#initiate a replay buffer\n",
    "#Replay buffer stores 10,000 timesteps worth data\n",
    "\n",
    "from collections import deque\n",
    "buffer_size = 10000\n",
    "replay_buffer= deque(maxlen=buffer_size)\n",
    "done = False\n",
    "image_state_buffer=[]\n",
    "state = env.reset()\n",
    "while len(replay_buffer)< buffer_size:\n",
    "    \n",
    "    action = random.choice([0,1,2,3,4,5])\n",
    "    next_state, reward, done, _,_ = env.step(action)\n",
    "    pre_process(state)\n",
    "    pre_process(next_state)\n",
    "    replay_buffer.append((state,action, reward, next_state, done))\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "    else:\n",
    "        state = next_state\n",
    "     \n",
    "            \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f72018",
   "metadata": {},
   "outputs": [],
   "source": [
    "##A function to update the replay buffer\n",
    "def update_replay(new_buf,replay_buffer):\n",
    "    arr={}\n",
    "    for i in range(0,100):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            num=random.randint(0,10000)\n",
    "            while(num in arr.values()):\n",
    "                num=random.randint(0,10000)\n",
    "            arr[i]=num\n",
    "            new_tensor = new_buf[i]\n",
    "            del replay_buffer[new_tensor]\n",
    "            replay_buffer.insert(num,new_tensor)\n",
    "            \n",
    "        except:\n",
    "            arr[i]=num\n",
    "            new_tensor = new_buf[i]\n",
    "            del replay_buffer[num]\n",
    "            replay_buffer.insert(num,new_tensor)\n",
    "            \n",
    "    return replay_buffer\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cab0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a deep neural network model for training using pyTorch\n",
    "#model for Q-val determination\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class image_model(nn.Module):\n",
    "    def __init__(self,actions):\n",
    "        self.actions = action\n",
    "        super(image_model,self).__init__()\n",
    "        self.conv_flat_ANN_stack = nn.Sequential(\n",
    "                 nn.Conv2d(3,32,8,stride=4),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Conv2d(32,64,4,stride=2),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Conv2d(64,64,3),\n",
    "                 nn.Flatten(),\n",
    "                 nn.Linear(22528,256),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Linear(256,actions),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Linear(actions,actions)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "            return self.conv_flat_ANN_stack(x)\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3cace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = image_model(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b090a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "##created 2nd network for finding target values\n",
    "import copy\n",
    "def update_Target_model():\n",
    "    target_model = copy.deepcopy(Model)\n",
    "    return target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6853dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net = update_Target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prepare batch to be loaded into Q-learning model and target network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Forward pass next_state from the batch into target network\n",
    "##Use max function to find maximum output Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d96898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "##Data loader\n",
    "#Lets say i am running 10 iterations then for every 1 out 0f 10 iteration there should be a new dataset in data loader\n",
    "#batch size 100\n",
    "class loaddat():\n",
    "    def __init__(self):\n",
    "        self.l_actions=[]\n",
    "        self.l_current=[]\n",
    "        self.l_reward=[]\n",
    "        self.l_next=[]\n",
    "        self.batch_size=10\n",
    "        \n",
    "    def data_loader(self,replay_buffer):\n",
    "        self.l_actions=[]\n",
    "        self.l_current=[]\n",
    "        self.l_reward=[]\n",
    "        self.l_next=[]\n",
    "        \n",
    "        for i in range(0,self.batch_size):\n",
    "            x = random.randint(0,len(replay_buffer)-1)\n",
    "            self.l_current.append(replay_buffer[x][0])\n",
    "            self.l_actions.append(replay_buffer[x][1])\n",
    "            self.l_next.append(replay_buffer[x][3])\n",
    "            self.l_reward.append(replay_buffer[x][2])\n",
    "\n",
    "        return self.l_current,self.l_actions,self.l_next,self.l_reward\n",
    "obj1 = loaddat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Forward pass into Q-Net\n",
    "class forward_Q(image_model):\n",
    "    def __init__(self):\n",
    "        super().__init__(actions)\n",
    "        self.Y=[]\n",
    "        self.y = torch.zeros(1,6)\n",
    "        self.q_val=[]\n",
    "    def q_val_pred(self,dl,aa):\n",
    "        self.Y=[]\n",
    "        self.y = torch.zeros(1,6)\n",
    "        self.q_val=[]\n",
    "        \n",
    "        for i in range(0,len(dl)):\n",
    "            \n",
    "            try:\n",
    "                dl[i] = torch.tensor(dl[i],dtype =torch.float32)\n",
    "                dl[i] = torch.permute(dl[i],(2,0,1)).unsqueeze(0)\n",
    "            except:\n",
    "                dl[i] = torch.tensor(dl[i][0],dtype =torch.float32)\n",
    "                dl[i] = torch.permute(dl[i],(2,0,1)).unsqueeze(0)\n",
    "                \n",
    "            self.Y.append(super().forward(dl[i]))\n",
    "                \n",
    "        for i in range(0,len(self.Y)):\n",
    "            self.Y[i]=torch.tensor(self.Y[i])\n",
    "        for i in range(0,len(self.Y)):\n",
    "            self.y = torch.cat([self.y,self.Y[i]],0)\n",
    "            \n",
    "        self.y = self.y[1:11]\n",
    "   #predicted q-value from Q-Net     \n",
    "        for i in range(0,len(aa)):\n",
    "            self.q_val.append(self.y[i][aa[i]])\n",
    "        self.q_val=torch.tensor(self.q_val,requires_grad=True)##predicted Q val is ready\n",
    "        return self.q_val\n",
    "    \n",
    "obj2 = forward_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65158449",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Forward pass next_state from the batch into T-learning model\n",
    "##choose maximum of network output\n",
    "#Find target value T by calling comp_targ_val\n",
    "class forward_T_net(image_model):\n",
    "    def __init__(self):\n",
    "        super().__init__(actions)\n",
    "        self.gamma=0.8\n",
    "    \n",
    "    def comp_targ_val(self,reward,max_q):\n",
    "        return reward+(self.gamma*max_q)\n",
    "    \n",
    "    def target(self,ndl,r,target_net):\n",
    "        self.T_output=[]\n",
    "        for i in range(0,len(ndl)):\n",
    "         \n",
    "                ndl[i] = torch.tensor(ndl[i],dtype =torch.float32)\n",
    "                ndl[i] = torch.permute(ndl[i],(2,0,1)).unsqueeze(0)\n",
    "                self.T_output.append(target_net.forward(ndl[i]))\n",
    "    \n",
    "        self.T=[]\n",
    "        for i in range(0,len(self.T_output)):\n",
    "            self.T.append(torch.max(self.T_output[i]))\n",
    "        self.T=torch.tensor(self.T)##targeted value from Target network\n",
    "        self.T_val =[]\n",
    "        for i in range(0,len(self.T)):\n",
    "            self.T_val.append(self.comp_targ_val(r[i],self.T[i]))\n",
    "        self.T_val = torch.tensor(self.T_val,requires_grad=True)##Now Targeted value is ready\n",
    "        return self.T_val\n",
    "    \n",
    "    \n",
    "obj3 = forward_T_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631733a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compute loss and backpropogate loss\n",
    "##Use MSE loss\n",
    "import torch.optim as optim\n",
    "# create your optimizer\n",
    "loss= nn.MSELoss()\n",
    "optimizer = optim.SGD(Model.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "   # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6192a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "##initial Main loop for training Q-Net\n",
    "iterations = 10000\n",
    "for i in range(0,iterations):\n",
    "    dl,aa,ndl,r = obj1.data_loader(replay_buffer)#call everything at once\n",
    "    q_val = obj2.q_val_pred(dl,aa)\n",
    "    if i==0 or i%100==0 :\n",
    "        \n",
    "        T_val = obj3.target(ndl,r,target_net)#Forward pass in T-Network\n",
    "        \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    LOSS = loss(q_val,T_val)\n",
    "    LOSS.backward()\n",
    "    optimizer.step() #back_propagate(Loss)\n",
    "    if(i%2000==0):\n",
    "        print(\"Loss:{}\".format(LOSS))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc04c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/sidhantdas/Documents/weigths\"\n",
    "torch.save(Model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0610f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Post training\n",
    "srb = deque()\n",
    "srb.clear()\n",
    "rb = deque(maxlen =10000)\n",
    "rb.clear()\n",
    "episodes = 10000\n",
    "new_buf = deque(maxlen = 100)\n",
    "new_buf.clear()\n",
    "score=0\n",
    "for game in range(0,episodes):\n",
    "    state= env.reset()\n",
    "    done=False\n",
    "    ctr=0\n",
    "    err_ctr = 0\n",
    "    while not done: \n",
    "                state = env.render()\n",
    "                if n>e or score==0 :\n",
    "                    action = random.choice([0,1,2,3,4,5])\n",
    "                else:\n",
    "                    a_state = torch.tensor(state,dtype =torch.float32)\n",
    "                    a_state = torch.permute(a_state,(2,0,1)).unsqueeze(0)\n",
    "                    output = Model.forward(a_state)\n",
    "                    action = np.argmax(output.detach().numpy)\n",
    "                n_state, reward, done, cond, info = env.step(action)\n",
    "                score+=reward\n",
    "                try:\n",
    "                    srb.append((state,action,reward,n_state))\n",
    "                except:\n",
    "                    srb.append((state.numpy(),action,reward,n_state))  \n",
    "                    state =n_state\n",
    "                try:\n",
    "                    next_state=pre_process(srb[ctr][3])\n",
    "                    state =pre_process(srb[ctr][0])\n",
    "                    if len(new_buf) < 100: \n",
    "                                       new_buf.append((state,action, reward, next_state, done))\n",
    "                                       \n",
    "                        \n",
    "                except:\n",
    "                    next_state=pre_process(srb[ctr][3][0])\n",
    "                    state =pre_process(srb[ctr][0][0])\n",
    "                    if len(new_buf) < 100:\n",
    "                                       new_buf.append((state,action, reward, next_state, done))\n",
    "                print(\"Episode:{},Score:{},Action:{} \".format(game,score,action))\n",
    "                ctr+=1\n",
    "                    \n",
    "\n",
    "                if(game%10==0 and game>0):\n",
    "                    #update replay buffer\n",
    "                    iterations = 10000\n",
    "                    rb = update_replay(new_buf,replay_buffer)\n",
    "                    for i in range(0,iterations):\n",
    "                    \n",
    "                    #Call in data_loader\n",
    "                        dl,aa,ndl,r=obj1.data_loader(rb)\n",
    "                    #call To train the Q-Net and T-net\n",
    "                        q_val = obj2.q_val_pred(dl,aa)\n",
    "                        if i==0 or i%100==0 :\n",
    "                            T_val = obj3.target(ndl,r,target_net)#Forward pass in T-Network\n",
    "                        optimizer.zero_grad()   # zero the gradient buffers\n",
    "                        LOSS = loss(q_val,T_val)\n",
    "                        LOSS.backward()\n",
    "                        optimizer.step() #back_propagate(Loss)\n",
    "                        if(game%20000==0):\n",
    "                            print(\"episode:{},Loss:{}\".format(episode,LOSS))\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
